* 추가 시도
 - Batch_size를 data_set 크기의 약수로 조정해서 0.02의 성능 향상을 얻음.
 - 초기에는 xlm-roberta로 학습을 진행하여 accuracy 0.6010을 얻음. 이후 피어세션에서 xlm-roberta-large에 대해서도 알게 되고 모델 변경을 통해 하이퍼파라미터 조정 없이 0.1510의 성능 향상을 얻음.

* 시도했으나 잘 되지 않았던 것들
 - batch_size를 기존의 16에서 8, 4, 2, 1로 감소시키며 학습을 진행했으나 오히려 성능이 떨어짐.
 - Learning rate를 낮추면 약간의 성능 향상이 있었지만 크게 낮추면 학습이 오래 걸려서 중간에 중단되는 이슈가 발생함 (메모리 용량 문제라고 생각함)
 - Overfitting test를 위해 학습진행도(check_point)에 따른 점수를 plot하여 최적화된 check_point를 찾으려고 함. 점수는 우상향그래프를 그림 -> train data가 너무 적어서 모델이 underfitting 상태였다고 생각함.
